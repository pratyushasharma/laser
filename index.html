
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>LASER</title>

    <!-- Bootstrap Core CSS -->
    <link href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" rel="stylesheet" type="text/css">

    <!-- Fonts -->
    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>

    <script src="js/jquery-1.10.2.js"></script>

    <!-- Custom Theme CSS -->
    <link href="css/grayscale.css" rel="stylesheet">
    <link href="css/timeline.css" rel="stylesheet">
    <link href="css/custom.css" rel="stylesheet">

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-custom">

        <!-- Some js animation -->
    <script>
        $(document).ready(function () {
                $("#talks_container").animate( {height: 15+'%'} );
        });
   </script>

    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->

                    <li class="page-scroll">
                        <a href="#page-top">Home</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#about">Results</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#posts">Team</a>
                    </li>
                    <li class="page-scroll">
                        <a href="https://github.com/pratyushasharma/laser" target="_blank">Code</a>
                    </li>
                    <li class="page-scroll">
                        <a href="https://arxiv.org/pdf/2312.13558.pdf" target="_blank">Paper</a>
                    </li>
                    <li class="page-scroll">
                        <a href="bib.txt" target="_blank">Bib</a>
                    </li>

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

     <section class="intro" style="background:none; background-color:white; border-bottom: 2px solid #888888;">
        <div class="intro-body">
            <div class="container">
        		<div class="row">
                     <div class="row" style="margin-top:10%; color:black; text-align:left; font-size:normal;">
        				        <h3 style="text-transform:none;" class="greenc">
                          <img src="images/laser.png" class="img-fluid" style="max-width: 50px; height: auto;"/> &nbsp; LASER: Layer SElective Rank-Reduction</h3>


                         <div class="col-md-12">

                             <p>LASER (LAyer SElective Rank-Reduction) is an intervention strategy for Large Language Models that was introduced
                             in the <a href="https://arxiv.org/pdf/2312.13558.pdf" target="_blank"><b>Paper</b></a> <i>The Truth Is In There:
                                     Improving Reasoning in Language Models with Layer-Selective Rank Reduction, (Sharma, Ash, and Misra, arXiv 2023)</i>.
                                 As the name suggests, LASER replaces selected weight matrices in an LLM with their low-rank approximation (which can be
                                 thought of as a way to compress information). The key surprising finding of the paper was that given a task, if we do
                                 LASER interventions properly, these reductions improve the performance of the LLM on that task, at times by 20-30 percentage points.
                                 Figure below visualizes LASER intervention in an LLM.
                             </p>

                             <img src="images/main.png" style="margin-top: 10px; width:80%; height: auto; margin-left: 10%; padding-bottom:50px;"/>

                             <p>The findings also show that improvements typically come from performing LASER in the MLP weight matrices in the latter half of the LLM. These
                                 results also didnt seem restricted to just LLM and were observed in
                                 <a href="https://arxiv.org/pdf/2106.01345.pdf" target="_blank">Decision Transformers</a> in an RL task.</p>

                             <p>There are lots of open questions in this space and several possible ways to extend these results. A goal of this webpage is to
                             contain a leaderboard with results on various benchmarks, and LLMs along with evaluating different modifications to LASER.
                             The code is open source (MIT license) and we welcome contributions. The <a href="https://github.com/pratyushasharma/laser" target="_blank"><b>GitHub Page</b></a>
                                 contains instructions for running LASER. We also provide a short installation snippet below:</p>
                             <p>
                                  <pre>
                                    # Clone the Laser code<br/>
                                    git clone https://github.com/pratyushasharma/laser.git<br/><br/>

                                    # (Optional) create a conda environment.<br/>
                                    conda create -n Laser python=3.8 -y<br/>
                                    conda activate Laser<br/><br/>

                                    # Install Requirements<br/>
                                    pip3 install -r requirements.txt<br/>
                                    pip install -e .<br/><br/>

                                    # Run a sample experiment (E.g., try GPTJ on Bios Gender dataset with a chosen LASER intervention)<br/>
                                    python3 intervention_llama2_bios.py --lname fc_in --rate 9.9 --lnum 26

                                  </pre>
                             </p>

                            <p><b>Does LASER require training?</b> To perform a single LASER intervention, you do need to select 3 scalar hyperparameters:
                            the layer to edit, the parameter type to edit and the amount of reduction to do. We typically find that doing significant reduction on
                                the later layers (often times the last) of MLP parameters often works. Most recently, we found that doing this on Phi-1.5 LLM
                            on the CounterFact dataset, immediately gave a 5-6 percentage point improvement without any fine-tuning. However, this is not always the case.
                            See Table 3 for list of optimal hyperparameters.</p>

                             <p><b>Can LASER reduce memory cost?</b> LASER can indeed reduce memory footprint. Given a nxn matrix whose rank is reduced to 1% of maximum rank, we get a memory
                                 reduction down to roughly 2% of the orignal. However, currently the code doesnt support this memory reduction feature. We aim to support it soon.</p>

                             <p><b>Can LASER be applied to a (some LLM name) LLM?</b> One can apply LASER to any LLM. We will release a tutorial soon to show how to do so.
                             In fact, it can also be applied to other transformer architectures, and in principle, any deep neural network. Our code, however, currently only
                             supports Llama2, GPT-J, Phi-1.5, Decision Transformer and Roberta. If your LLM is on HuggingFace, then it should be very easy to modify the code
                             to apply LASER to the LLM. Finally, while we observed improvements with LASER across several benchmarks and LLMs, there is currently no mathematical
                             guarantee that this will always happen. We encourage you try and paste results and add to the community knowledge (see the next question).</p>

                            <p><b>I have a new result/found a way to improve LASER</b> We welcome such contributions. Please send us either an email or
                                contribute to <a href="https://github.com/pratyushasharma/laser/discussions" target="_blank">Github discussion</a> and we will add the results to the leaderboard below unless you tell us not to.</p>
                         </div>
                     </div>
        		</div>
            </div>
        </div>

    </section>

    <section id="about" class="container content-section text-center" style="padding-top:50px; width:100%; padding-bottom:30px; border-bottom: 2px solid #888888;">
        <div class="row">
        	<h2 class="greenc">Results with LASER</h2>

               <div class="row" style="margin-left: 10%; margin-right: 10%; text-align: left;">

                   <p style="margin-top: 10px;">Following are results with LASER on various public benchmarks. If you have a result and want to be cited here, then please
                       raise a Github issue <a>here</a> or send an email to one of the authors. We specially welcome modifications to LASER. </p>
                   
                   <p><b>CounterFact dataset (<a href="https://github.com/pratyushasharma/laser/blob/main/scripts/get_counterfact.py" target="_blank">script</a> to get dataset)</b></p>

                   <table class="table col-md-12 results">
                          <thead class="thead-dark">
                            <tr>
                              <th scope="col">Base Model Name</th>
                              <th scope="col">Base Model Accuracy (Log Loss)</th>
                              <th scope="col">Laser Accuracy Log Loss)</th>
                              <th scope="col">Laser Hyperparameters (&tau;, &ell;, &rho;)</th>
                              <th scope="col">Credit and Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Roberta (12 layers, 355M)</td>
                              <td>17.3 (5.78)</td>
                              <td>19.3 (5.43)</td>
                              <td>[U<sub>in</sub>, 8, 0.8]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>GPT-J (28 layers, 6B)</td>
                              <td>13.1 (5.78)</td>
                              <td>24.0 (5.05)</td>
                              <td>[U<sub>in</sub>, 27, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>Llama2 (32 layers, 7B)</td>
                              <td>35.6 (3.61)</td>
                              <td>37.6 (3.49)</td>
                              <td>[U<sub>in</sub>, 28, 0.05]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                          </tbody>
                    </table>

                    <p><b>Hotpot QA (<a href="https://github.com/pratyushasharma/laser/blob/main/src/dataset_utils/hotpot.py" target="_blank">script</a> to get dataset)</b></p>

                   <table class="table col-md-12 results">
                          <thead class="thead-dark">
                            <tr>
                              <th scope="col">Base Model Name</th>
                              <th scope="col">Base Model Accuracy (Log Loss)</th>
                              <th scope="col">Laser Accuracy (Log Loss)</th>
                              <th scope="col">Laser Hyperparameters (&tau;, &ell;, &rho;)</th>
                              <th scope="col">Credit and Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Roberta (12 layers, 355M)</td>
                              <td> 6.1 (10.99)</td>
                              <td> 6.7 (10.55)</td>
                              <td>[U<sub>out</sub>, 2, 0.4]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>GPT-J (28 layers, 6B)</td>
                              <td>19.6 (3.40)</td>
                              <td>19.5 (3.39)</td>
                              <td>[U<sub>in</sub>, 27, 0.1]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>Llama2 (32 layers, 7B)</td>
                              <td>16.5 (3.15)</td>
                              <td>17.2 (2.97)</td>
                              <td> [U<sub>in</sub>, 27, 0.2]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                          </tbody>
                    </table>

                    <p><b>Fever (<a href="https://github.com/pratyushasharma/laser/blob/main/src/dataset_utils/fever.py" target="_blank">script</a> to get dataset)</b></p>

                   <table class="table col-md-12 results">
                          <thead class="thead-dark">
                            <tr>
                              <th scope="col">Base Model Name</th>
                              <th scope="col">Base Model Accuracy (Log Loss)</th>
                              <th scope="col">Laser Accuracy (Log Loss)</th>
                              <th scope="col">Laser Hyperparameters (&tau;, &ell;, &rho;)</th>
                              <th scope="col">Credit and Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Roberta (12 layers, 355M)</td>
                              <td>50.0 (2.5)</td>
                              <td>52.3 (1.76)</td>
                              <td>[U<sub>in</sub>, 3, 0.4]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>GPT-J (28 layers, 6B)</td>
                              <td>50.2 (1.24)</td>
                              <td>56.2 (1.27)</td>
                              <td>[U<sub>in</sub>, 24, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>Llama2 (32 layers, 7B)</td>
                              <td>59.3 (1.02)</td>
                              <td>64.5 (0.91)</td>
                              <td>[U<sub>in</sub>, 30, 0.2]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                          </tbody>
                    </table>

                    <p><b>Bios Gender (<a href="https://github.com/pratyushasharma/laser/blob/main/src/dataset_utils/bias_in_bios.py" target="_blank">script</a> to get dataset)</b></p>

                   <table class="table col-md-12 results">
                          <thead class="thead-dark">
                            <tr>
                              <th scope="col">Base Model Name</th>
                              <th scope="col">Base Model Accuracy (Log Loss)</th>
                              <th scope="col">Laser Accuracy (Log Loss)</th>
                              <th scope="col">Laser Hyperparameters (&tau;, &ell;, &rho;)</th>
                              <th scope="col">Credit and Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Roberta (12 layers, 355M)</td>
                              <td>87.5 (0.87)</td>
                              <td>93.7 (1.13)</td>
                              <td>[U<sub>in</sub>, 9, 0.9]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>GPT-J (28 layers, 6B)</td>
                              <td>70.9 (3.86)</td>
                              <td>97.5 (4.20)</td>
                              <td>[U<sub>in</sub>, 14, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>Llama2 (32 layers, 7B)</td>
                              <td>75.5 (3.48)</td>
                              <td>88.4 (2.98)</td>
                              <td>[U<sub>in</sub>, 24, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                          </tbody>
                    </table>

                    <p><b>Bios Profession (<a href="https://github.com/pratyushasharma/laser/blob/main/src/dataset_utils/bias_in_bios.py" target="_blank">script</a> to get dataset)</b></p>

                   <table class="table col-md-12 results">
                          <thead class="thead-dark">
                            <tr>
                              <th scope="col">Base Model Name</th>
                              <th scope="col">Base Model Accuracy (Log Loss)</th>
                              <th scope="col">Laser Accuracy (Log Loss)</th>
                              <th scope="col">Laser Hyperparameters (&tau;, &ell;, &rho;)</th>
                              <th scope="col">Credit and Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Roberta (12 layers, 355M)</td>
                              <td>64.5 (4.91)</td>
                              <td>72.5 (6.44)</td>
                              <td>[U<sub>in</sub>, 3, 0.9]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>GPT-J (28 layers, 6B)</td>
                              <td>75.6 (4.64)</td>
                              <td>82.1 (4.91)</td>
                              <td>[U<sub>in</sub>, 18, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>Llama2 (32 layers, 7B)</td>
                              <td>85.0 (4.19)</td>
                              <td>86.7 (4.05)</td>
                              <td>[U<sub>out</sub>, 30, 0.4]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                          </tbody>
                    </table>

                    <p><b>TruthfulQA (<a href="https://github.com/pratyushasharma/laser/blob/main/src/dataset_utils/truthfulqa.py" target="_blank">script</a> to get dataset)</b></p>

                   <table class="table col-md-12 results">
                          <thead class="thead-dark">
                            <tr>
                              <th scope="col">Base Model Name</th>
                              <th scope="col">Base Model Accuracy (Log Loss)</th>
                              <th scope="col">Laser Accuracy (Log Loss)</th>
                              <th scope="col">Laser Hyperparameters (&tau;, &ell;, &rho;)</th>
                              <th scope="col">Credit and Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Roberta (12 layers, 355M)</td>
                              <td>56.2 (1.60)</td>
                              <td>56.2 (1.42)</td>
                              <td>[U<sub>in</sub>, 0, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>GPT-J (28 layers, 6B)</td>
                              <td>54.9 (1.02)</td>
                              <td>55.6 (1.01)</td>
                              <td>[U<sub>in</sub>, 7, 0.8]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>Llama2 (32 layers, 7B)</td>
                              <td>50.5 (0.95)</td>
                              <td>56.2 (1.04)</td>
                              <td>[U<sub>in</sub>, 30, 0.05]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                          </tbody>
                    </table>

                   <p><b>BigBench-Epistemic Reasoning (<a href="https://github.com/pratyushasharma/laser/blob/main/src/dataset_utils/bigbench.py" target="_blank">script</a> to get dataset)</b></p>

                   <table class="table col-md-12 results">
                          <thead class="thead-dark">
                            <tr>
                              <th scope="col">Base Model Name</th>
                              <th scope="col">Base Model Accuracy (Log Loss)</th>
                              <th scope="col">Laser Accuracy (Log Loss)</th>
                              <th scope="col">Laser Hyperparameters (&tau;, &ell;, &rho;)</th>
                              <th scope="col">Credit and Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Roberta (12 layers, 355M)</td>
                              <td>37.1 (9.39)</td>
                              <td>41.8 (6.80)</td>
                              <td>[U<sub>out</sub>, 1, 0.4]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>GPT-J (28 layers, 6B)</td>
                              <td>37.1 (0.74)</td>
                              <td>38.3 (0.62)</td>
                              <td>[U<sub>in</sub>, 26, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>Llama2 (32 layers, 7B)</td>
                              <td>44.8 (0.78)</td>
                              <td>63.4 (0.73)</td>
                              <td>[U<sub>out</sub>, 28, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                          </tbody>
                    </table>

                   <p><b>BigBench-WikidataQA (<a href="https://github.com/pratyushasharma/laser/blob/main/src/dataset_utils/bigbench.py" target="_blank">script</a> to get dataset)</b></p>

                   <table class="table col-md-12 results">
                          <thead class="thead-dark">
                            <tr>
                              <th scope="col">Base Model Name</th>
                              <th scope="col">Base Model Accuracy (Log Loss)</th>
                              <th scope="col">Laser Accuracy (Log Loss)</th>
                              <th scope="col">Laser Hyperparameters (&tau;, &ell;, &rho;)</th>
                              <th scope="col">Credit and Description</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>Roberta (12 layers, 355M)</td>
                              <td>28.0 (9.07)</td>
                              <td>30.7 (7.69)</td>
                              <td>[U<sub>in</sub>, 7, 0.4]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>GPT-J (28 layers, 6B)</td>
                              <td>51.8 (3.52)</td>
                              <td>65.9 (2.86)</td>
                              <td>[U<sub>in</sub>, 27, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                            <tr>
                              <td>Llama2 (32 layers, 7B)</td>
                              <td>59.5 (2.40)</td>
                              <td>62.0 (2.31)</td>
                              <td>[U<sub>in</sub>, 27, 0.01]</td>
                              <td>from the original LASER paper</td>
                            </tr>
                          </tbody>
                    </table>
               </div>
        </div>
    </section>

    <section id="posts" class="intro"  class="one-edge-shadow" style="background:none; background-color:white; color:black; border:black; padding-top:60px; padding-bottom:40px; width:100%;  border-bottom: 2px solid #888888;">
        <div class="intro-body">
            <h2>Team</h2>

            <div class="container" id="bio" style="text-align:left;">

                <p style="margin-top:20px;">Authors are listed in the order in the paper below.</p>

                <div class="row">
                  <div class="col-md-4">
                    <div class="thumbnail">
                      <a href="https://pratyushasharma.github.io/">
                        <img src="images/pratyusha.jpg" alt="Lights" style="height:200px;">
                        <div class="caption">
                          <p class="name">Pratyusha Sharma<br/>Massachusetts Institute of Technology</p>
                        </div>
                      </a>
                    </div>
                  </div>
                  <div class="col-md-4">
                    <div class="thumbnail">
                      <a href="https://www.jordantash.com/">
                        <img src="images/jordan_t_ash.jpg" alt="Nature" style="height:200px;">
                        <div class="caption">
                          <p class="name">Jordan T. Ash<br/>Microsoft Research New York</p>
                        </div>
                      </a>
                    </div>
                  </div>
                  <div class="col-md-4">
                    <div class="thumbnail">
                      <a href="https://www.dipendramisra.com">
                        <img src="images/dipendra.jpeg" alt="Fjords" style="height:200px;">
                        <div class="caption">
                          <p class="name">Dipendra Misra<br/>Microsoft Research New York</p>
                        </div>
                      </a>
                    </div>
                  </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Core JavaScript Files -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
    <script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/grayscale.js"></script>

</body>
</html>